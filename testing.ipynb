{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4d0d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Tensor import Tensor\n",
    "from optim import SGD\n",
    "import numpy as np\n",
    "\n",
    "params1 = Tensor(np.random.rand(3,2), requires_grad=False)\n",
    "params2 = Tensor(np.random.rand(3,2))\n",
    "optim = SGD([params1, params2])\n",
    "out = params1 * params2\n",
    "out = out.mean()\n",
    "optim.zero_grad()\n",
    "out.backward()\n",
    "optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93841a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.nn.Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09abd76c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTEXT LEN PARENTS : 2\n",
      "CONTEXT LEN PARENTS : 2\n",
      "CONTEXT LEN PARENTS : 2\n",
      "CONTEXT LEN PARENTS : 2\n",
      "CONTEXT LEN PARENTS : 2\n",
      "CONTEXT LEN PARENTS : 2\n",
      "CONTEXT LEN PARENTS : 2\n",
      "CONTEXT LEN PARENTS : 2\n",
      "CONTEXT LEN PARENTS : 2\n",
      "CONTEXT LEN PARENTS : 2\n"
     ]
    }
   ],
   "source": [
    "from Tensor import Tensor\n",
    "\n",
    "\n",
    "x = Tensor([5])\n",
    "b = Tensor([2])\n",
    "c =  Tensor([2.3])\n",
    "d = Tensor([-3.4])\n",
    "\n",
    "for i in range(10):\n",
    "    x = x + 3\n",
    "    x.backward()\n",
    "    x.grad = None\n",
    "    print(\"CONTEXT LEN PARENTS :\", len(x._ctx.saved_tensors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a20ee1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.3683: 100%|██████████| 1257/1257 [00:01<00:00, 653.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TARGET:  5\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(23)\n",
    "np.random.seed(23)\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "n_samples = len(digits.images)\n",
    "data = digits.images.reshape((n_samples, -1))\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, digits.target, test_size=0.3, shuffle=False\n",
    ")\n",
    "\n",
    "def get_weights(in_dim, out_dim):\n",
    "    # Xavier/Glorot initialization\n",
    "    std = np.sqrt(2.0 / (in_dim + out_dim))\n",
    "    return np.random.normal(0, std, (in_dim, out_dim)).astype(np.float32)\n",
    "\n",
    "\n",
    "class Linear:\n",
    "    def __init__(self, in_dim: int, out_dim: int):\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "\n",
    "        weights = get_weights(self.in_dim, self.out_dim)\n",
    "        self.weight = nn.Parameter(torch.tensor(weights), requires_grad=True)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        return x.matmul(self.weight)\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim: int, out_dim: int):\n",
    "        super().__init__()\n",
    "        self.l1 = Linear(in_dim, 128)\n",
    "        self.l2 = Linear(128, out_dim)\n",
    "\n",
    "        self.l1.weight.retain_grad()\n",
    "        self.l2.weight.retain_grad()\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        x = self.l1.forward(x)\n",
    "        x = x.relu()\n",
    "        return self.l2.forward(x)\n",
    "\n",
    "model = MLP(8 * 8, 10)\n",
    "lr = 0.001\n",
    "# sgd = torch.optim.SGD([model.l1.weight, model.l2.weight], lr=lr)\n",
    "count = 0\n",
    "# X_train = X_train[:1]\n",
    "# y_train = y_train[:1]\n",
    "for epoch in range(1):\n",
    "    progress_bar = tqdm(zip(X_train, y_train), total=len(y_train), desc=\"loss: 0.0\")\n",
    "    for sample, target in progress_bar:\n",
    "        sample = Tensor(sample)\n",
    "\n",
    "        one_hot_target = np.zeros(10)\n",
    "        one_hot_target[target] = 1\n",
    "        target = Tensor(one_hot_target)\n",
    "\n",
    "        logits = model.forward(sample.reshape(1, -1))\n",
    "        logits.retain_grad()\n",
    "\n",
    "        probs = logits.softmax(-1)\n",
    "        probs.retain_grad()\n",
    "\n",
    "        safe_probs = probs + 1e-8\n",
    "        log_probs = safe_probs.log()\n",
    "        log_probs.retain_grad()\n",
    "        loss_dot = target.dot(log_probs.reshape(-1))\n",
    "        loss = -loss_dot\n",
    "\n",
    "\n",
    "        model.l1.weight.grad = None\n",
    "        model.l2.weight.grad = None\n",
    "        model.l1.weight.retain_grad()\n",
    "        model.l2.weight.retain_grad()\n",
    "\n",
    "        debug = logits\n",
    "        debug.retain_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        count += 1\n",
    "        # if count == 1:\n",
    "        #     print(\"TYPE: \", debug.dtype)\n",
    "        #     print(\"debug var item: \", debug)\n",
    "        #     print(\"debug var GRAD: \", debug.grad)\n",
    "        #     print(\"LOSS: \", loss)\n",
    "        #     print(\"model.l1 GRAD: \", model.l1.weight.grad)\n",
    "        #     break\n",
    "\n",
    "        model.l1.weight = model.l1.weight - lr * model.l1.weight.grad\n",
    "        model.l2.weight = model.l2.weight - lr * model.l2.weight.grad\n",
    "        progress_bar.set_description_str(f\"loss: {float(loss.item()):.4f}\")\n",
    "\n",
    "sample_idx = 2\n",
    "print(\"TARGET: \", y_test[sample_idx])\n",
    "\n",
    "x = Tensor(X_test[sample_idx])\n",
    "res = model.forward(x)\n",
    "print(torch.argmax(res.detach()).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af929aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Torch softmax:\n",
      " [[0.30096766 0.07694629 0.13723414 0.48485196]\n",
      " [0.6287442  0.03655759 0.25120136 0.08349688]\n",
      " [0.11495472 0.19216523 0.14720124 0.5456788 ]]\n",
      "My softmax:\n",
      " [[0.30096766 0.0769463  0.13723414 0.48485196]\n",
      " [0.6287442  0.03655759 0.25120136 0.08349688]\n",
      " [0.11495472 0.19216523 0.14720125 0.5456788 ]]\n",
      "Difference:\n",
      " 1.4901161e-08\n",
      "Torch grad:\n",
      " [[0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00]\n",
      " [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00]\n",
      " [6.851835e-09 1.145394e-08 8.773878e-09 3.252499e-08]]\n",
      "My grad:\n",
      " [[ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00]\n",
      " [-7.4952148e-08 -4.3580037e-09 -2.9945536e-08 -9.9536033e-09]\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00]]\n",
      "Grad difference:\n",
      " 7.495215e-08\n",
      "----------------------------------------\n",
      "Iteration 1\n",
      "Torch softmax:\n",
      " [[0.7749224  0.08281929 0.09003924 0.05221913]\n",
      " [0.27410856 0.01154876 0.6604546  0.0538881 ]\n",
      " [0.20853183 0.11811778 0.65403473 0.01931558]]\n",
      "My softmax:\n",
      " [[0.7749224  0.08281928 0.09003923 0.05221913]\n",
      " [0.27410856 0.01154876 0.6604546  0.0538881 ]\n",
      " [0.20853186 0.11811779 0.6540348  0.01931558]]\n",
      "Difference:\n",
      " 5.9604645e-08\n",
      "Torch grad:\n",
      " [[0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [1.24294655e-08 7.04036829e-09 3.89835080e-08 1.15129828e-09]]\n",
      "My grad:\n",
      " [[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "Grad difference:\n",
      " 3.8983508e-08\n",
      "----------------------------------------\n",
      "Iteration 2\n",
      "Torch softmax:\n",
      " [[0.09582314 0.13741359 0.01716832 0.7495949 ]\n",
      " [0.06554094 0.1697562  0.65131414 0.11338879]\n",
      " [0.02803653 0.03253944 0.14017822 0.7992458 ]]\n",
      "My softmax:\n",
      " [[0.09582314 0.13741359 0.01716832 0.7495949 ]\n",
      " [0.06554094 0.1697562  0.65131414 0.11338878]\n",
      " [0.02803653 0.03253945 0.14017822 0.7992458 ]]\n",
      "Difference:\n",
      " 7.450581e-09\n",
      "Torch grad:\n",
      " [[0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      " [1.6711076e-09 1.9395019e-09 8.3552729e-09 4.7638760e-08]]\n",
      "My grad:\n",
      " [[ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00]\n",
      " [-7.8130888e-09 -2.0236516e-08 -7.7642696e-08 -1.3516996e-08]\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00]]\n",
      "Grad difference:\n",
      " 7.7642696e-08\n",
      "----------------------------------------\n",
      "Iteration 3\n",
      "Torch softmax:\n",
      " [[0.680849   0.17612992 0.1253625  0.01765854]\n",
      " [0.26005164 0.24068993 0.315932   0.18332633]\n",
      " [0.22423135 0.14536946 0.06296833 0.56743085]]\n",
      "My softmax:\n",
      " [[0.6808491  0.17612994 0.1253625  0.01765854]\n",
      " [0.26005167 0.24068996 0.31593204 0.18332635]\n",
      " [0.22423135 0.14536946 0.06296833 0.56743085]]\n",
      "Difference:\n",
      " 5.9604645e-08\n",
      "Torch grad:\n",
      " [[0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      " [3.1000571e-08 2.8692476e-08 3.7662030e-08 2.1854202e-08]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]]\n",
      "My grad:\n",
      " [[-8.1163535e-08 -2.0996325e-08 -1.4944375e-08 -2.1050621e-09]\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00]\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00]]\n",
      "Grad difference:\n",
      " 8.1163535e-08\n",
      "----------------------------------------\n",
      "Iteration 4\n",
      "Torch softmax:\n",
      " [[0.20743535 0.3251249  0.0728461  0.39459366]\n",
      " [0.20529339 0.06395402 0.16320956 0.56754315]\n",
      " [0.31092185 0.07079687 0.41403943 0.2042418 ]]\n",
      "My softmax:\n",
      " [[0.20743535 0.32512486 0.0728461  0.39459366]\n",
      " [0.20529336 0.06395401 0.16320953 0.5675431 ]\n",
      " [0.31092185 0.07079687 0.41403943 0.2042418 ]]\n",
      "Difference:\n",
      " 5.9604645e-08\n",
      "Torch grad:\n",
      " [[ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00]\n",
      " [-2.4472879e-08 -7.6239131e-09 -1.9456095e-08 -6.7656416e-08]\n",
      " [ 1.8532386e-08  4.2198223e-09  2.4678673e-08  1.2173760e-08]]\n",
      "My grad:\n",
      " [[0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      " [1.8532386e-08 4.2198223e-09 2.4678673e-08 1.2173760e-08]]\n",
      "Grad difference:\n",
      " 6.7656416e-08\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from Tensor import Tensor\n",
    "\n",
    "for i in range(5):\n",
    "    # Generate the same random values\n",
    "    np.random.seed(i)\n",
    "    arr = np.random.randn(3, 4).astype(np.float32)\n",
    "\n",
    "    # Torch tensor\n",
    "    torch_t = torch.tensor(arr, requires_grad=True)\n",
    "    torch_softmax = torch_t.softmax(-1)\n",
    "    torch_loss = torch_softmax.sum()\n",
    "    torch_loss.backward()\n",
    "    \n",
    "    # Custom Tensor\n",
    "    my_t = Tensor(arr, requires_grad=True)\n",
    "    my_softmax = my_t.softmax()\n",
    "    my_loss = my_softmax.sum()\n",
    "    my_loss.backward()\n",
    "\n",
    "    print(f\"Iteration {i}\")\n",
    "    print(\"Torch softmax:\\n\", torch_softmax.detach().numpy())\n",
    "    print(\"My softmax:\\n\", my_softmax.item)\n",
    "    print(\"Difference:\\n\", np.abs(torch_softmax.detach().numpy() - my_softmax.item).max())\n",
    "    print(\"Torch grad:\\n\", torch_t.grad.numpy())\n",
    "    print(\"My grad:\\n\", my_t.grad)\n",
    "    print(\"Grad difference:\\n\", np.abs(torch_t.grad.numpy() - my_t.grad).max())\n",
    "    print(\"-\" * 40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simplegrad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
